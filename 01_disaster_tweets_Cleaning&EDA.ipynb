{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = Path.cwd()\n",
    "DATA_DIR = Path.cwd()/'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR/'raw/train.csv')\r\n",
    "test = pd.read_csv(DATA_DIR/'raw/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Keywords in Test But not in Train\n"
     ]
    }
   ],
   "source": [
    "train = train.fillna('missing')\n",
    "test = test.fillna('missing')\n",
    "train['keyword'] = train['keyword'].apply(lambda x: x.replace('%20', ' ').lower())\n",
    "test['keyword'] = test['keyword'].apply(lambda x: x.replace('%20', ' ').lower())\n",
    "\n",
    "words = len([word for word in test.keyword.unique() if word not in train.keyword.unique()])\n",
    "print(f'{words} Keywords in Test But not in Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "missing                  2533\n",
       "usa                       104\n",
       "new york                   75\n",
       "united states              50\n",
       "london                     49\n",
       "                         ... \n",
       "sisterhood                  1\n",
       "cimerak - pangandaran       1\n",
       "ondo                        1\n",
       "u.s                         1\n",
       "the void, u.s.a             1\n",
       "Name: location, Length: 3234, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Location is too fine-grained and maynot be useful \n",
    "train.location.fillna('missing').apply(lambda x: x.lower()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ablaze               36\n",
       "accident             35\n",
       "aftershock           34\n",
       "airplane accident    35\n",
       "ambulance            38\n",
       "                     ..\n",
       "wounded              37\n",
       "wounds               33\n",
       "wreck                37\n",
       "wreckage             39\n",
       "wrecked              39\n",
       "Name: keyword, Length: 222, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May need to stem some of these words eg. wounded/wounds\n",
    "train.keyword.fillna('missing').value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So no imbalance class problem\n",
    "train.target.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "def lemmatize_text(text): \n",
    "    return (' '.join([token.lemma_ for token in nlp(text.lower()) if (token.text not in stop_words) & (not token.is_punct)]).replace('# ', ''))\n",
    "\n",
    "def extract_hashtag(tweet):\n",
    "    split_tweet = tweet.split('#')\n",
    "    if len(split_tweet)>1:\n",
    "        hashtags = [hashtag.split()[0] for hashtag in split_tweet[1:] if len(hashtag)>=1]\n",
    "        return ' '.join(sorted(hashtags))\n",
    "    else:\n",
    "        return 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize keywords\n",
    "lemmatized_kw = [lemmatize_text(x) for x in train.keyword.unique()]\n",
    "lemmatized_dict = dict(zip(train.keyword.unique(), lemmatized_kw))\n",
    "train['keyword_processed'] = train.keyword.map(lemmatized_dict)\n",
    "train['keyword_processed'] = train['keyword_processed'].apply(lambda x: ''.join(x.split(' ')))\n",
    "\n",
    "test['keyword_processed'] = test.keyword.map(lemmatized_dict)\n",
    "test['keyword_processed'] = test['keyword_processed'].apply(lambda x: ''.join(x.split(' ')))\n",
    "\n",
    "## Lemmatize main body text\n",
    "train['text_processed'] = train.text.apply(lambda x: lemmatize_text(x))\n",
    "test['text_processed'] = test.text.apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "# Extracting Hashtags\n",
    "train['hashtags'] = train.text.apply(lambda x: extract_hashtag(x))\n",
    "test['hashtags'] = test.text.apply(lambda x: extract_hashtag(x))\n",
    "\n",
    "# Using regex to clean up different links\n",
    "train['text_processed'] = train.text_processed.apply(lambda x: re.sub('http://[aA-zZ0-9\\/\\.]+',  'http', x).replace('http http', 'http'))\n",
    "test['text_processed'] = test.text_processed.apply(lambda x:  re.sub('http://[aA-zZ0-9\\/\\.]+',  'http', x).replace('http http', 'http'))\n",
    "\n",
    "# Drop duplicates tweets for train test\n",
    "train = train.drop_duplicates('text_processed').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_processed</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>13,000 people receive wildfire evacuation orde...</td>\n",
       "      <td>wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>Alaska wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6970</th>\n",
       "      <td>10860</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>a siren just went off and it wasn't the Forney...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>siren go forney tornado warning</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6971</th>\n",
       "      <td>10862</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>official quarantine place alabama home possibl...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6972</th>\n",
       "      <td>10863</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>worldnews fall powerline g link tram update fi...</td>\n",
       "      <td>WorldNews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973</th>\n",
       "      <td>10864</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>flip walmart bomb evacuate stay tune blow</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6974</th>\n",
       "      <td>10866</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>suicide bomber kill 15 saudi security site mos...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6975 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword location  \\\n",
       "0         1  missing  missing   \n",
       "1         4  missing  missing   \n",
       "2         5  missing  missing   \n",
       "3         6  missing  missing   \n",
       "4         7  missing  missing   \n",
       "...     ...      ...      ...   \n",
       "6970  10860  missing  missing   \n",
       "6971  10862  missing  missing   \n",
       "6972  10863  missing  missing   \n",
       "6973  10864  missing  missing   \n",
       "6974  10866  missing  missing   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "6970  a siren just went off and it wasn't the Forney...       1   \n",
       "6971  Officials say a quarantine is in place at an A...       1   \n",
       "6972  #WorldNews Fallen powerlines on G:link tram: U...       1   \n",
       "6973  on the flip side I'm at Walmart and there is a...       1   \n",
       "6974  Suicide bomber kills 15 in Saudi security site...       1   \n",
       "\n",
       "     keyword_processed                                     text_processed  \\\n",
       "0                 miss               deed reason earthquake allah forgive   \n",
       "1                 miss              forest fire near la ronge sask canada   \n",
       "2                 miss  resident ask shelter place notify officer evac...   \n",
       "3                 miss  13,000 people receive wildfire evacuation orde...   \n",
       "4                 miss  got send photo ruby alaska smoke wildfire pour...   \n",
       "...                ...                                                ...   \n",
       "6970              miss                    siren go forney tornado warning   \n",
       "6971              miss  official quarantine place alabama home possibl...   \n",
       "6972              miss  worldnews fall powerline g link tram update fi...   \n",
       "6973              miss          flip walmart bomb evacuate stay tune blow   \n",
       "6974              miss  suicide bomber kill 15 saudi security site mos...   \n",
       "\n",
       "              hashtags  \n",
       "0           earthquake  \n",
       "1              missing  \n",
       "2              missing  \n",
       "3            wildfires  \n",
       "4     Alaska wildfires  \n",
       "...                ...  \n",
       "6970           missing  \n",
       "6971           missing  \n",
       "6972         WorldNews  \n",
       "6973           missing  \n",
       "6974           missing  \n",
       "\n",
       "[6975 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.text_processed.str.contains('')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(DATA_DIR/'interim/train.pkl')\n",
    "test.to_pickle(DATA_DIR/'interim/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = np.array([token.vector for token in nlp(train.text_processed[0])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 96)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA And Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1. Comparing Keywords Prevalence of Actual Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_ohe = pd.get_dummies(train['keyword'],drop_first=True)\n",
    "train_merged = train.merge(kw_ohe, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body bag           0.000000\n",
       "aftershock         0.000000\n",
       "body bags          0.025000\n",
       "ruin               0.027027\n",
       "blazing            0.030303\n",
       "                     ...   \n",
       "suicide bomber     0.964286\n",
       "suicide bombing    0.965517\n",
       "wreckage           1.000000\n",
       "debris             1.000000\n",
       "derailment         1.000000\n",
       "Length: 221, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_val = train_merged.iloc[:, 8:].values\n",
    "num_disaster = np.array([np.sum(train.target.values * kw_val[:, num]) for num in range(kw_val.shape[1])])\n",
    "\n",
    "kw_count = train_merged.iloc[:, 8:].sum()\n",
    "kw_proba = pd.Series(num_disaster/kw_count.values, index=kw_count.index)\n",
    "kw_proba.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating Keywords into either groups that has high probability with 0 or 1\n",
    "low_kw = list(kw_proba.sort_values()[0:50].index)\n",
    "high_kw = list(kw_proba.sort_values()[-50:].index)\n",
    "main_kw = low_kw + high_kw\n",
    "all_kw = kw_proba.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2. Body Text Prevalence of Actual Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(1,1), min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(2,2), min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = count.fit_transform(train['text_processed'])\n",
    "vector = pd.DataFrame(np.c_[vectorized.toarray(), np.array(train.target)], columns = count.get_feature_names()+['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prevalence(df): \n",
    "    features = df.drop('target', axis=1)\n",
    "    active = np.sum(np.array(features) * train.target.to_numpy().reshape(-1,1), axis=0)\n",
    "    word_counts = np.sum(np.array(features), axis=0)\n",
    "    series = pd.Series(active/word_counts, index=features.columns)\n",
    "    return series.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mh370',\n",
       " 'legionnaire',\n",
       " 'mosque',\n",
       " 'debris',\n",
       " 'migrant',\n",
       " 'derailment',\n",
       " 'northern',\n",
       " 'severe',\n",
       " 'hiroshima',\n",
       " 'bombing',\n",
       " 'warning',\n",
       " 'bomber',\n",
       " 'typhoon',\n",
       " 'spill',\n",
       " 'saudi',\n",
       " 'flooding',\n",
       " 'wildfire',\n",
       " 'reunion',\n",
       " 'malaysia',\n",
       " 'outbreak',\n",
       " 'japan',\n",
       " 'hundred',\n",
       " 'reuter',\n",
       " 'soudelor',\n",
       " 'aircraft',\n",
       " 'suspect',\n",
       " 'helicopter',\n",
       " 'atomic',\n",
       " 'california',\n",
       " 'suicide',\n",
       " '70',\n",
       " 'anniversary',\n",
       " 'east',\n",
       " 'crew',\n",
       " 'amid',\n",
       " 'confirm',\n",
       " 'island',\n",
       " 'plane',\n",
       " 'earthquake',\n",
       " 'bridge',\n",
       " 'calgary',\n",
       " 'pakistan',\n",
       " 'victim',\n",
       " 'near',\n",
       " 'kill',\n",
       " 'rd',\n",
       " 'mark',\n",
       " 'airplane',\n",
       " 'km',\n",
       " 'rescuer',\n",
       " 'west',\n",
       " 'drought',\n",
       " 'thunderstorm',\n",
       " 'project',\n",
       " 'train',\n",
       " 'hailstorm',\n",
       " 'terrorist',\n",
       " 'officer',\n",
       " '40',\n",
       " 'issue',\n",
       " 'murder',\n",
       " 'evacuation',\n",
       " 'case',\n",
       " 'massacre',\n",
       " 'county',\n",
       " 'india',\n",
       " 'south',\n",
       " 'area',\n",
       " 'report',\n",
       " 'north',\n",
       " 'affect',\n",
       " 'close',\n",
       " 'obama',\n",
       " '05',\n",
       " 'boat',\n",
       " 'alarm',\n",
       " 'million',\n",
       " 'search',\n",
       " '08',\n",
       " 'vehicle',\n",
       " 'continue',\n",
       " '15',\n",
       " '06',\n",
       " 'volcano',\n",
       " 'forest',\n",
       " 'displace',\n",
       " 'weather',\n",
       " '00',\n",
       " 'police',\n",
       " 'oil',\n",
       " 'evacuate',\n",
       " 'fire',\n",
       " 'electrocute',\n",
       " 'siren',\n",
       " 'loud',\n",
       " 'twitter',\n",
       " 'wo',\n",
       " 'share',\n",
       " 'maybe',\n",
       " 'desolate',\n",
       " 'bar',\n",
       " 'youtube',\n",
       " 'explode',\n",
       " 'gon',\n",
       " 'gt',\n",
       " 'demolish',\n",
       " 'beautiful',\n",
       " 'fan',\n",
       " 'pay',\n",
       " 'cross',\n",
       " 'night',\n",
       " 'ûªt',\n",
       " 'blue',\n",
       " 'stand',\n",
       " 'eye',\n",
       " 'girl',\n",
       " 'add',\n",
       " 'yes',\n",
       " 'thank',\n",
       " 'nt',\n",
       " 'think',\n",
       " 'online',\n",
       " 'pretty',\n",
       " 'win',\n",
       " 'blow',\n",
       " 'guy',\n",
       " 'good',\n",
       " 'lot',\n",
       " 'kid',\n",
       " 'hot',\n",
       " 'sink',\n",
       " 'sit',\n",
       " 'try',\n",
       " 'radio',\n",
       " 'haha',\n",
       " 'appear',\n",
       " 'gbbo',\n",
       " 'trust',\n",
       " 'phone',\n",
       " 'check',\n",
       " 'bang',\n",
       " 'game',\n",
       " 'sensor',\n",
       " 'friend',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'star',\n",
       " 'body',\n",
       " 'okay',\n",
       " 'deluge',\n",
       " 'blaze',\n",
       " 'ass',\n",
       " 'ask',\n",
       " 'avalanche',\n",
       " 'meltdown',\n",
       " 'inundate',\n",
       " 'desolation',\n",
       " 'play',\n",
       " 'feel',\n",
       " 'mom',\n",
       " 'wanna',\n",
       " 'wreck',\n",
       " 'word',\n",
       " 'want',\n",
       " 'fuck',\n",
       " 'bloody',\n",
       " 'twister',\n",
       " 'lady',\n",
       " 'tweet',\n",
       " 'flatten',\n",
       " 'stock',\n",
       " 'soon',\n",
       " 'lava',\n",
       " 'lord',\n",
       " 'fucking',\n",
       " 'buy',\n",
       " 'obliteration',\n",
       " 'trouble',\n",
       " 'reddit',\n",
       " 'well',\n",
       " 'fun',\n",
       " 'let',\n",
       " 'favorite',\n",
       " 'content',\n",
       " 'bleed',\n",
       " 'space',\n",
       " 'sorry',\n",
       " 'wrong',\n",
       " 'curfew',\n",
       " 'listen',\n",
       " 'scream',\n",
       " 'ball',\n",
       " 'job',\n",
       " 'probably',\n",
       " 'love',\n",
       " 'lol',\n",
       " 'obliterate',\n",
       " 'hey',\n",
       " 'harm',\n",
       " 'crush',\n",
       " 'upheaval',\n",
       " 'stretcher',\n",
       " 'armageddon',\n",
       " 'music',\n",
       " 'cry',\n",
       " 'panic',\n",
       " 'book',\n",
       " 'ruin',\n",
       " 'happy',\n",
       " 'shoulder',\n",
       " 'blight',\n",
       " 'self',\n",
       " 'song',\n",
       " 'traumatised',\n",
       " 'nowplaye',\n",
       " 'ebay',\n",
       " 'cake',\n",
       " 'bag']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevalence = calc_prevalence(vector)\n",
    "\n",
    "def \n",
    "list(prevalence.index[(prevalence>0.75)]) + list(prevalence.index[(prevalence<0.25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kids got Disney version of the game Operation only 2 AA batteries? I swear my old version had like 8 Ds and would nearly electrocute you.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.text_processed.str.contains('electrocute')].iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_processed</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>3211</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>susinesses are deluged with invoices. Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>susinesse deluge invoice stlnd colour shape li...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>3216</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with invoices. Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge invoice stand colo r shape lzk...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>3218</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with invoices. Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge invoice stand colour shape lik...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>3222</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with invokces. Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge invokce stand colour shape.and...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>3235</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with inroices.|Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge inroices.|make stand colour sh...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>3237</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with invoices. Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge invoice stand oup colour shame...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>3239</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with invzices. Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge invzice stand colour shape it'...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>3240</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with invoices. Make you...</td>\n",
       "      <td>1</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge invoice stand colour shape lik...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>3243</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are deluged with invoices. Make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business deluge invoice stand ogt colomr shape...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>3244</td>\n",
       "      <td>deluged</td>\n",
       "      <td>missing</td>\n",
       "      <td>Businesses are|deluged with invoices. Make y u...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "      <td>business are|deluge invoice y ur stand colour ...</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  keyword location  \\\n",
       "2116  3211  deluged  missing   \n",
       "2118  3216  deluged  missing   \n",
       "2120  3218  deluged  missing   \n",
       "2123  3222  deluged  missing   \n",
       "2128  3235  deluged  missing   \n",
       "2130  3237  deluged  missing   \n",
       "2131  3239  deluged  missing   \n",
       "2132  3240  deluged  missing   \n",
       "2135  3243  deluged  missing   \n",
       "2136  3244  deluged  missing   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2116  susinesses are deluged with invoices. Make you...       0   \n",
       "2118  Businesses are deluged with invoices. Make you...       0   \n",
       "2120  Businesses are deluged with invoices. Make you...       0   \n",
       "2123  Businesses are deluged with invokces. Make you...       0   \n",
       "2128  Businesses are deluged with inroices.|Make you...       0   \n",
       "2130  Businesses are deluged with invoices. Make you...       0   \n",
       "2131  Businesses are deluged with invzices. Make you...       0   \n",
       "2132  Businesses are deluged with invoices. Make you...       1   \n",
       "2135  Businesses are deluged with invoices. Make you...       0   \n",
       "2136  Businesses are|deluged with invoices. Make y u...       0   \n",
       "\n",
       "     keyword_processed                                     text_processed  \\\n",
       "2116            deluge  susinesse deluge invoice stlnd colour shape li...   \n",
       "2118            deluge  business deluge invoice stand colo r shape lzk...   \n",
       "2120            deluge  business deluge invoice stand colour shape lik...   \n",
       "2123            deluge  business deluge invokce stand colour shape.and...   \n",
       "2128            deluge  business deluge inroices.|make stand colour sh...   \n",
       "2130            deluge  business deluge invoice stand oup colour shame...   \n",
       "2131            deluge  business deluge invzice stand colour shape it'...   \n",
       "2132            deluge  business deluge invoice stand colour shape lik...   \n",
       "2135            deluge  business deluge invoice stand ogt colomr shape...   \n",
       "2136            deluge  business are|deluge invoice y ur stand colour ...   \n",
       "\n",
       "     hashtags  \n",
       "2116  missing  \n",
       "2118  missing  \n",
       "2120  missing  \n",
       "2123  missing  \n",
       "2128  missing  \n",
       "2130  missing  \n",
       "2131  missing  \n",
       "2132  missing  \n",
       "2135  missing  \n",
       "2136  missing  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.text_processed.str.contains('pay pile')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
