{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "WORK_DIR = Path.cwd()\n",
    "DATA_DIR = Path.cwd()/'data'\n",
    "OUT_DIR = Path.cwd()/'output'\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(DATA_DIR/'interim/train.pkl').reset_index(drop=True)\n",
    "test = pd.read_pickle(DATA_DIR/'interim/test.pkl').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array(train)\n",
    "np.delete(array, [1,2,3,4,5], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_processed</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>length</th>\n",
       "      <th>word counts</th>\n",
       "      <th>capital</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>missing</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>missing</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>13,000 people receive wildfire evacuation orde...</td>\n",
       "      <td>wildfires</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>miss</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>alaska wildfires</td>\n",
       "      <td>88</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                               text  \\\n",
       "0   1  missing  missing  our deeds are the reason of this #earthquake m...   \n",
       "1   4  missing  missing             forest fire near la ronge sask. canada   \n",
       "2   5  missing  missing  all residents asked to 'shelter in place' are ...   \n",
       "3   6  missing  missing  13,000 people receive #wildfires evacuation or...   \n",
       "4   7  missing  missing  just got sent this photo from ruby #alaska as ...   \n",
       "\n",
       "   target keyword_processed  \\\n",
       "0       1              miss   \n",
       "1       1              miss   \n",
       "2       1              miss   \n",
       "3       1              miss   \n",
       "4       1              miss   \n",
       "\n",
       "                                      text_processed          hashtags  \\\n",
       "0               deed reason earthquake allah forgive        earthquake   \n",
       "1              forest fire near la ronge sask canada           missing   \n",
       "2  resident ask shelter place notify officer evac...           missing   \n",
       "3  13,000 people receive wildfire evacuation orde...         wildfires   \n",
       "4  got send photo ruby alaska smoke wildfire pour...  alaska wildfires   \n",
       "\n",
       "   length  word counts  capital  num_hashtags  num_tags  \n",
       "0      69           13        0            10         0  \n",
       "1      38            7        0             7         0  \n",
       "2     133           22        0             7         0  \n",
       "3      65            9        0             9         0  \n",
       "4      88           17        0            16         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text_features_col(X, selected_col = 'text'):\n",
    "    df = X.copy()\n",
    "    df['length'] = train[selected_col].apply(lambda x: len(x))\n",
    "    df['word counts'] = train[selected_col].apply(lambda x: len(x.split(' ')))\n",
    "    df['capital'] = train[selected_col].apply(lambda x: sum(map(str.isupper, x)))\n",
    "    df['num_hashtags'] = train['hashtags'].apply(lambda x: len(x))\n",
    "    df['num_tags'] = train[selected_col].apply(lambda x: x.count('@'))\n",
    "\n",
    "    return df\n",
    "\n",
    "features_transformer = FunctionTransformer(generate_text_features_col, kw_args={\"selected_col\": 'text'})\n",
    "features_transformer.fit_transform(train)\n",
    "features_transformer.transform(train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHE_Transformer(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, categories=None, col='keyword_processed', to_array=False):\n",
    "        self.categories = categories\n",
    "        self.col = col\n",
    "        self.to_array = to_array\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.categories = X.loc[:, self.col].unique()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        ohe = pd.get_dummies(X[self.col],drop_first=True)\n",
    "        ohe = ohe.T.reindex(self.categories).T.fillna(0)\n",
    "        ohe.columns = ['kw_'+col for col in ohe.columns]\n",
    "        df = pd.concat([X, ohe], axis=1)\n",
    "\n",
    "        if self.to_array: \n",
    "            df = np.array(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "ohe_kw = OHE_Transformer()\n",
    "ohe_kw.fit_transform(train)\n",
    "test_transform = ohe_kw.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Vectorizer(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, col = 'text_processed', tfidf = False, **kwargs):\n",
    "        self.col = col\n",
    "        self.tfidf = tfidf\n",
    "        \n",
    "        if self.tfidf:\n",
    "            self.vectorizer = TfidfVectorizer(**kwargs)\n",
    "        else:\n",
    "            self.vectorizer = CountVectorizer(**kwargs)\n",
    "                \n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X[self.col])\n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.tfidf:\n",
    "            vectorizer = TfidfVectorizer(vocabulary = self.vocab)\n",
    "        else:\n",
    "            vectorizer = CountVectorizer(vocabulary = self.vocab)\n",
    "        words = vectorizer.fit_transform(X[self.col]).toarray()\n",
    "        words_df = pd.DataFrame(words, \n",
    "                                columns=['body_'+col for col in vectorizer.get_feature_names()],\n",
    "                                index=X.index)\n",
    "\n",
    "        merged = pd.concat([X, words_df], axis=1)\n",
    "        return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_miss</th>\n",
       "      <th>kw_ablaze</th>\n",
       "      <th>kw_accident</th>\n",
       "      <th>kw_aftershock</th>\n",
       "      <th>kw_airplaneaccident</th>\n",
       "      <th>kw_ambulance</th>\n",
       "      <th>kw_annihilate</th>\n",
       "      <th>kw_annihilation</th>\n",
       "      <th>kw_apocalypse</th>\n",
       "      <th>kw_armageddon</th>\n",
       "      <th>...</th>\n",
       "      <th>kw_violentstorm</th>\n",
       "      <th>kw_volcano</th>\n",
       "      <th>kw_warzone</th>\n",
       "      <th>kw_weapon</th>\n",
       "      <th>kw_whirlwind</th>\n",
       "      <th>kw_wildfire</th>\n",
       "      <th>kw_windstorm</th>\n",
       "      <th>kw_wound</th>\n",
       "      <th>kw_wreck</th>\n",
       "      <th>kw_wreckage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      kw_miss  kw_ablaze  kw_accident  kw_aftershock  kw_airplaneaccident  \\\n",
       "0         1.0        0.0          0.0            0.0                  0.0   \n",
       "1         1.0        0.0          0.0            0.0                  0.0   \n",
       "2         1.0        0.0          0.0            0.0                  0.0   \n",
       "3         1.0        0.0          0.0            0.0                  0.0   \n",
       "4         1.0        0.0          0.0            0.0                  0.0   \n",
       "...       ...        ...          ...            ...                  ...   \n",
       "3258      1.0        0.0          0.0            0.0                  0.0   \n",
       "3259      1.0        0.0          0.0            0.0                  0.0   \n",
       "3260      1.0        0.0          0.0            0.0                  0.0   \n",
       "3261      1.0        0.0          0.0            0.0                  0.0   \n",
       "3262      1.0        0.0          0.0            0.0                  0.0   \n",
       "\n",
       "      kw_ambulance  kw_annihilate  kw_annihilation  kw_apocalypse  \\\n",
       "0              0.0            0.0              0.0            0.0   \n",
       "1              0.0            0.0              0.0            0.0   \n",
       "2              0.0            0.0              0.0            0.0   \n",
       "3              0.0            0.0              0.0            0.0   \n",
       "4              0.0            0.0              0.0            0.0   \n",
       "...            ...            ...              ...            ...   \n",
       "3258           0.0            0.0              0.0            0.0   \n",
       "3259           0.0            0.0              0.0            0.0   \n",
       "3260           0.0            0.0              0.0            0.0   \n",
       "3261           0.0            0.0              0.0            0.0   \n",
       "3262           0.0            0.0              0.0            0.0   \n",
       "\n",
       "      kw_armageddon  ...  kw_violentstorm  kw_volcano  kw_warzone  kw_weapon  \\\n",
       "0               0.0  ...              0.0         0.0         0.0        0.0   \n",
       "1               0.0  ...              0.0         0.0         0.0        0.0   \n",
       "2               0.0  ...              0.0         0.0         0.0        0.0   \n",
       "3               0.0  ...              0.0         0.0         0.0        0.0   \n",
       "4               0.0  ...              0.0         0.0         0.0        0.0   \n",
       "...             ...  ...              ...         ...         ...        ...   \n",
       "3258            0.0  ...              0.0         0.0         0.0        0.0   \n",
       "3259            0.0  ...              0.0         0.0         0.0        0.0   \n",
       "3260            0.0  ...              0.0         0.0         0.0        0.0   \n",
       "3261            0.0  ...              0.0         0.0         0.0        0.0   \n",
       "3262            0.0  ...              0.0         0.0         0.0        0.0   \n",
       "\n",
       "      kw_whirlwind  kw_wildfire  kw_windstorm  kw_wound  kw_wreck  kw_wreckage  \n",
       "0              0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "1              0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "2              0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "3              0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "4              0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "...            ...          ...           ...       ...       ...          ...  \n",
       "3258           0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "3259           0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "3260           0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "3261           0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "3262           0.0          0.0           0.0       0.0       0.0          0.0  \n",
       "\n",
       "[3263 rows x 182 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_features(X, selected_cols = None, to_array=False):\n",
    "    df = X.copy()\n",
    "    if selected_cols is None: \n",
    "        selected_cols = df.columns\n",
    "        \n",
    "    initial_cols = ['id', 'keyword', 'location', 'text', 'target', 'keyword_processed',\n",
    "                    'text_processed', 'hashtags']\n",
    "    select_cols = [col for col in selected_cols if col not in initial_cols]\n",
    "    df = df[select_cols]\n",
    "    \n",
    "    if to_array:\n",
    "        df = np.array(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "feature_selector = FunctionTransformer(select_features, kw_args={\"to_array\": False})\n",
    "feature_selector.fit_transform(train)\n",
    "feature_selector.transform(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVector(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, col = 'text', spacy_ver = None, num_components=None):\n",
    "        self.col = col\n",
    "        self.spacy = spacy_ver\n",
    "        self.pca = None \n",
    "        self.num_components = num_components \n",
    "\n",
    "        if self.spacy is None:\n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "            self.spacy = nlp\n",
    "                \n",
    "    def fit(self, X, y=None):\n",
    "        self.pca = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        text_vector = X[self.col].apply(lambda x: self.spacy(x).vector)\n",
    "        array = np.array([list(arr) for arr in list(text_vector)])\n",
    "        \n",
    "        if self.num_components is not None: \n",
    "            if self.pca is not None: \n",
    "                pca = self.pca \n",
    "                array = pca.transform(array)\n",
    "\n",
    "            else:\n",
    "                pca = PCA(n_components=self.num_components)\n",
    "                array = pca.fit_transform(array)\n",
    "\n",
    "            self.pca = pca\n",
    "            self.explained_variance_ratio_ = pca.explained_variance_ratio_\n",
    "        comp_df = pd.DataFrame(array, columns = [f'comp_{i+1}' for i in range(array.shape[1])])\n",
    "        merged = pd.concat([X, comp_df], axis=1)\n",
    "        return merged\n",
    "    \n",
    "    \n",
    "class WordVector_Small(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, selected_columns = 'text_processed', spacy_ver = None):\n",
    "        self.col = selected_columns\n",
    "        self.spacy = spacy_ver\n",
    "        self.pca = None \n",
    "\n",
    "        if self.spacy is None:\n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "            self.spacy = nlp\n",
    "                \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        text_vector = X['text_processed'].apply(lambda x: self.spacy(x).vector)\n",
    "        array = np.array([list(arr) for arr in list(text_vector)])\n",
    "        comp_df = pd.DataFrame(array, columns = [f'comp_{i+1}' for i in range(array.shape[1])])\n",
    "        merged = pd.concat([X, comp_df], axis=1)\n",
    "        return merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "word_vec = WordVector(spacy_ver = nlp, num_components=50)\n",
    "\n",
    "train_vec = word_vec.fit_transform(train)\n",
    "test_vec = word_vec.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec.to_pickle(DATA_DIR/'interim/train_word_vector.pkl')\n",
    "test_vec.to_pickle(DATA_DIR/'interim/test_word_vector.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Only Top Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prevalence(df): \n",
    "    features = df.drop('target', axis=1)\n",
    "    active = np.sum(np.array(features) * train.target.to_numpy().reshape(-1,1), axis=0)\n",
    "    word_counts = np.sum(np.array(features), axis=0)\n",
    "    series = pd.Series(active/word_counts, index=features.columns)\n",
    "    return series.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def export_high_prevalence(series, threshold):\n",
    "    return list(prevalence.index[(prevalence>(1-threshold))]) + list(prevalence.index[(prevalence<threshold)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(1,1), min_df=30)\n",
    "vectorized = count.fit_transform(train['text_processed'])\n",
    "vector = pd.DataFrame(np.c_[vectorized.toarray(), np.array(train.target)], columns = count.get_feature_names()+['target'])\n",
    "\n",
    "prevalence = calc_prevalence(vector)\n",
    "vocab_1 = export_high_prevalence(prevalence, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(2,2), min_df=15)\n",
    "vectorized = count.fit_transform(train['text_processed'])\n",
    "vector = pd.DataFrame(np.c_[vectorized.toarray(), np.array(train.target)], columns = count.get_feature_names()+['target'])\n",
    "\n",
    "prevalence = calc_prevalence(vector)\n",
    "vocab_2 = export_high_prevalence(prevalence, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_vec.drop('target', axis=1), train_vec['target'], test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword_processed</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>comp_1</th>\n",
       "      <th>comp_2</th>\n",
       "      <th>comp_3</th>\n",
       "      <th>...</th>\n",
       "      <th>body_way</th>\n",
       "      <th>body_weapon</th>\n",
       "      <th>body_wildfire</th>\n",
       "      <th>body_woman</th>\n",
       "      <th>body_work</th>\n",
       "      <th>body_world</th>\n",
       "      <th>body_wound</th>\n",
       "      <th>body_wreck</th>\n",
       "      <th>body_year</th>\n",
       "      <th>body_youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>9220</td>\n",
       "      <td>suicide bombing</td>\n",
       "      <td>USA</td>\n",
       "      <td>turkish troops killed in kurdish militant 'sui...</td>\n",
       "      <td>suicidebombing</td>\n",
       "      <td>turkish troop kill kurdish militant suicide at...</td>\n",
       "      <td>missing</td>\n",
       "      <td>-1.163644</td>\n",
       "      <td>0.863250</td>\n",
       "      <td>-0.026014</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>5502</td>\n",
       "      <td>flames</td>\n",
       "      <td>Fairy Tail!</td>\n",
       "      <td>@aisumage @akumareisu --just between gray and ...</td>\n",
       "      <td>flame</td>\n",
       "      <td>@aisumage @akumareisu --just gray ophelia red ...</td>\n",
       "      <td>missing</td>\n",
       "      <td>-0.776185</td>\n",
       "      <td>-0.118698</td>\n",
       "      <td>-0.562627</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>3493</td>\n",
       "      <td>derailed</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>#tubestrike derailed you? our #robertwelch cut...</td>\n",
       "      <td>derail</td>\n",
       "      <td>tubestrike derail robertwelch cutlery offer tr...</td>\n",
       "      <td>robertwelch tubestrike</td>\n",
       "      <td>-0.038096</td>\n",
       "      <td>-0.487312</td>\n",
       "      <td>0.670012</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>7643</td>\n",
       "      <td>pandemonium</td>\n",
       "      <td>California</td>\n",
       "      <td>truly a scene of chaos unprecedented in frenzy...</td>\n",
       "      <td>pandemonium</td>\n",
       "      <td>truly scene chaos unprecedented frenzy pandemo...</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.519658</td>\n",
       "      <td>-0.020307</td>\n",
       "      <td>-0.140782</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>9288</td>\n",
       "      <td>sunk</td>\n",
       "      <td>missing</td>\n",
       "      <td>everything has sunk in except the fact that i ...</td>\n",
       "      <td>sink</td>\n",
       "      <td>sink fact actually move state colorado tomorro...</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.938073</td>\n",
       "      <td>-0.632953</td>\n",
       "      <td>-0.397866</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4060</th>\n",
       "      <td>5769</td>\n",
       "      <td>forest fires</td>\n",
       "      <td>missing</td>\n",
       "      <td>#heartdisease u.s. forest service says spendin...</td>\n",
       "      <td>forestfire</td>\n",
       "      <td>heartdisease u.s forest service say spend half...</td>\n",
       "      <td>heartdisease</td>\n",
       "      <td>-0.438908</td>\n",
       "      <td>0.643492</td>\n",
       "      <td>-0.400172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>1945</td>\n",
       "      <td>burning buildings</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>i will never support looting or the burning of...</td>\n",
       "      <td>burnbuilding</td>\n",
       "      <td>support looting burning building see people fi...</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.029256</td>\n",
       "      <td>-1.224668</td>\n",
       "      <td>-0.205016</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>4940</td>\n",
       "      <td>exploded</td>\n",
       "      <td>Oakland, Ca</td>\n",
       "      <td>holy crap @kingmyth1999 my phone just exploded...</td>\n",
       "      <td>explode</td>\n",
       "      <td>holy crap @kingmyth1999 phone explode haha</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>-0.216649</td>\n",
       "      <td>0.980248</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>10771</td>\n",
       "      <td>wreckage</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>wreckage 'conclusively confirmed' as from mh37...</td>\n",
       "      <td>wreckage</td>\n",
       "      <td>wreckage conclusively confirm mh370 malaysia pm</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.332373</td>\n",
       "      <td>-0.087433</td>\n",
       "      <td>-0.313077</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>5117</td>\n",
       "      <td>fatal</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>apd investigating fatal shooting of 3 year old...</td>\n",
       "      <td>fatal</td>\n",
       "      <td>apd investigate fatal shooting 3 year old chil...</td>\n",
       "      <td>missing</td>\n",
       "      <td>-1.315902</td>\n",
       "      <td>0.718903</td>\n",
       "      <td>-0.419050</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6090 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            keyword        location  \\\n",
       "6445   9220    suicide bombing             USA   \n",
       "3870   5502             flames    Fairy Tail!    \n",
       "2431   3493           derailed  United Kingdom   \n",
       "5355   7643        pandemonium      California   \n",
       "6496   9288               sunk         missing   \n",
       "...     ...                ...             ...   \n",
       "4060   5769       forest fires         missing   \n",
       "1346   1945  burning buildings     Seattle, WA   \n",
       "3454   4940           exploded     Oakland, Ca   \n",
       "7533  10771           wreckage          Mumbai   \n",
       "3582   5117              fatal   Anchorage, AK   \n",
       "\n",
       "                                                   text keyword_processed  \\\n",
       "6445  turkish troops killed in kurdish militant 'sui...    suicidebombing   \n",
       "3870  @aisumage @akumareisu --just between gray and ...             flame   \n",
       "2431  #tubestrike derailed you? our #robertwelch cut...            derail   \n",
       "5355  truly a scene of chaos unprecedented in frenzy...       pandemonium   \n",
       "6496  everything has sunk in except the fact that i ...              sink   \n",
       "...                                                 ...               ...   \n",
       "4060  #heartdisease u.s. forest service says spendin...        forestfire   \n",
       "1346  i will never support looting or the burning of...      burnbuilding   \n",
       "3454  holy crap @kingmyth1999 my phone just exploded...           explode   \n",
       "7533  wreckage 'conclusively confirmed' as from mh37...          wreckage   \n",
       "3582  apd investigating fatal shooting of 3 year old...             fatal   \n",
       "\n",
       "                                         text_processed  \\\n",
       "6445  turkish troop kill kurdish militant suicide at...   \n",
       "3870  @aisumage @akumareisu --just gray ophelia red ...   \n",
       "2431  tubestrike derail robertwelch cutlery offer tr...   \n",
       "5355  truly scene chaos unprecedented frenzy pandemo...   \n",
       "6496  sink fact actually move state colorado tomorro...   \n",
       "...                                                 ...   \n",
       "4060  heartdisease u.s forest service say spend half...   \n",
       "1346  support looting burning building see people fi...   \n",
       "3454         holy crap @kingmyth1999 phone explode haha   \n",
       "7533    wreckage conclusively confirm mh370 malaysia pm   \n",
       "3582  apd investigate fatal shooting 3 year old chil...   \n",
       "\n",
       "                    hashtags    comp_1    comp_2    comp_3  ...  body_way  \\\n",
       "6445                 missing -1.163644  0.863250 -0.026014  ...         0   \n",
       "3870                 missing -0.776185 -0.118698 -0.562627  ...         0   \n",
       "2431  robertwelch tubestrike -0.038096 -0.487312  0.670012  ...         0   \n",
       "5355                 missing  0.519658 -0.020307 -0.140782  ...         0   \n",
       "6496                 missing  0.938073 -0.632953 -0.397866  ...         0   \n",
       "...                      ...       ...       ...       ...  ...       ...   \n",
       "4060            heartdisease -0.438908  0.643492 -0.400172  ...         0   \n",
       "1346                 missing  0.029256 -1.224668 -0.205016  ...         0   \n",
       "3454                 missing  0.563763 -0.216649  0.980248  ...         0   \n",
       "7533                 missing  0.332373 -0.087433 -0.313077  ...         0   \n",
       "3582                 missing -1.315902  0.718903 -0.419050  ...         0   \n",
       "\n",
       "      body_weapon  body_wildfire  body_woman  body_work  body_world  \\\n",
       "6445            0              0           0          0           0   \n",
       "3870            0              0           0          0           0   \n",
       "2431            0              0           0          0           0   \n",
       "5355            0              0           0          0           0   \n",
       "6496            0              0           0          0           0   \n",
       "...           ...            ...         ...        ...         ...   \n",
       "4060            0              0           0          0           0   \n",
       "1346            0              0           0          0           0   \n",
       "3454            0              0           0          0           0   \n",
       "7533            0              0           0          0           0   \n",
       "3582            0              0           0          0           0   \n",
       "\n",
       "      body_wound  body_wreck  body_year  body_youtube  \n",
       "6445           0           0          0             0  \n",
       "3870           0           0          0             0  \n",
       "2431           0           0          0             0  \n",
       "5355           0           0          0             0  \n",
       "6496           0           0          0             0  \n",
       "...          ...         ...        ...           ...  \n",
       "4060           0           0          0             0  \n",
       "1346           0           0          0             0  \n",
       "3454           0           0          0             0  \n",
       "7533           0           0          0             0  \n",
       "3582           0           0          1             0  \n",
       "\n",
       "[6090 rows x 199 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = Text_Vectorizer(tfidf=False, min_df=50,  ngram_range=(1,2))\n",
    "vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "                 ('extra_features', FunctionTransformer(generate_text_features_col, kw_args={\"selected_col\": 'text'})),\n",
    "                 ('ohe', OHE_Transformer()), \n",
    "                 #('vectorize1', Text_Vectorizer(sublinear_tf=True, tfidf=True, max_features=60000, min_df=1, norm='l2',  ngram_range=(1,2))),\n",
    "                 ('vectorize2', Text_Vectorizer(tfidf=False, min_df=10,  ngram_range=(1,2))),\n",
    "                 ('feature_selector', FunctionTransformer(select_features, kw_args={\"to_array\": False})), \n",
    "                 #('standard_scaler', StandardScaler()), \n",
    "                 #('pca', PCA(n_components=1000))\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortpipe= deepcopy(pipe)\n",
    "shortpipe.steps.pop(-1)\n",
    "shortpipe.steps.pop(-1)\n",
    "transform_train = shortpipe.fit_transform(X_train)\n",
    "transform_test = shortpipe.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984241628365069"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe = deepcopy(pipe)\n",
    "lr_pipe.steps.append(['logistic',  LogisticRegression(C=3, penalty='l1', solver='liblinear')])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "lr_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7491797900262467"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(n_splits=4, random_state=123)\n",
    "k_fold = cross_val_score(lr_pipe, X_test, y_test, cv=cv)\n",
    "k_fold.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8848932676518884"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha': 1,\n",
    " 'gamma': 1,\n",
    " 'learning_rate': 0.4,\n",
    " 'max_depth': 15,\n",
    " 'min_child_weight': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7248850952068286"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_pipe = deepcopy(pipe)\n",
    "xg_pipe.steps.append(['xgboost',  XGBClassifier(objective='binary:logistic', eval_metric ='error', **params)])\n",
    "xg_pipe.fit(X_train, y_train)\n",
    "xg_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5771498135101533"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(n_splits=4, random_state=123)\n",
    "k_fold = cross_val_score(xg_pipe, X_test, y_test, cv=cv)\n",
    "k_fold.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5686371100164204"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pipe = Pipeline([('extra_features', FunctionTransformer(generate_text_features_col, kw_args={\"selected_col\": 'text'})),\n",
    "                     ('ohe', OHE_Transformer()), \n",
    "                     ('vectorize1', Text_Vectorizer(min_df=40, tfidf = False, ngram_range=(1,1))),\n",
    "                     ('vectorize2', Text_Vectorizer(min_df=20, tfidf = False, ngram_range=(2,2))),\n",
    "                     ('feature_selector', FunctionTransformer(select_features, kw_args={\"to_array\": False})), \n",
    "                     ('standard_scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "X_train_xg = xg_pipe.fit_transform(X_train)\n",
    "X_test_xg = xg_pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_params =  {\"learning_rate\"    : [0.05, 0.15, 0.30 ],\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 2, 5],\n",
    "        'max_depth': [3, 6, 12], \n",
    "        'alpha': [1, 10, 100]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xg_pipe.steps.append(['xgboost',  XGBClassifier(objective='binary:logistic', eval_metric ='error')])\n",
    "search = GridSearchCV(XGBClassifier(objective='binary:logistic', eval_metric ='error'), xg_params, cv=3, n_jobs=6, verbose=3, refit=True)\n",
    "search.fit(X_train_xg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(test_vec.id, lr_pipe.predict(test_vec)), columns=['id', 'target']).set_index('id').to_csv(OUT_DIR/'logistic_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(test_vec.id, xg_pipe.predict(test_vec)), columns=['id', 'target']).set_index('id').to_csv(OUT_DIR/'xgboost_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'pca__n_components': [5, 15, 30, 45, 64],\n",
    "    'logistic__C': np.logspace(-4, 4, 4),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lr_pipe.predict(X_test), normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortpipe= deepcopy(pipe)\n",
    "shortpipe.steps.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(xg_pipe.steps[-1][-1])\n",
    "shap_values = explainer(shortpipe.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
